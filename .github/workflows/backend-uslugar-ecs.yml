name: Backend - Reuse existing Task Definition (ECR‚ÜíECS)

on:
  push:
    branches: [ "main" ]
    paths:
      - "uslugar/backend/**"
      - ".github/workflows/backend-uslugar-ecs.yml"

env:
  AWS_REGION: eu-north-1
  ECR_REPO_BACKEND: uslugar
  ECS_CLUSTER: apps-cluster
  ECS_SERVICE: uslugar-service-2gk1f1mv
  CONTAINER_NAME: uslugar
  # üîΩ Privatni subneti i SG za Fargate task koji ƒáe pokrenuti migracije
  MIGRATE_SUBNETS: subnet-0a00f97768705bbcf,subnet-01b67edfd00dc288c
  MIGRATE_SG: sg-084c1e49c9c77aff1

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions: { id-token: write, contents: read }

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      # üîë Generiraj package-lock.json ako ga nema (za npm ci u Dockerfileu)
      - name: Generate package-lock.json (backend)
        run: |
          cd uslugar/backend
          if [ ! -f package-lock.json ]; then
            echo "No package-lock.json found, generating..."
            npm i --package-lock-only
          fi
          head -n 20 package-lock.json || true

      - name: Build & Push image
        env:
          ECR_REGISTRY: ${{ steps.ecr-login.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -f uslugar/backend/Dockerfile.prod \
            -t $ECR_REGISTRY/${{ env.ECR_REPO_BACKEND }}:$IMAGE_TAG uslugar/backend
          docker push $ECR_REGISTRY/${{ env.ECR_REPO_BACKEND }}:$IMAGE_TAG
          echo "IMAGE_URI=$ECR_REGISTRY/${{ env.ECR_REPO_BACKEND }}:$IMAGE_TAG" >> $GITHUB_ENV

      - name: Install JQ
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Read current Task Definition
        run: |
          TD_ARN=$(aws ecs describe-services \
            --cluster "${{ env.ECS_CLUSTER }}" \
            --services "${{ env.ECS_SERVICE }}" \
            --query "services[0].taskDefinition" --output text)
          echo "Using task definition: $TD_ARN"
          aws ecs describe-task-definition --task-definition "$TD_ARN" \
            --query "taskDefinition" > td.json

          # Oƒçisti nedopu≈°tena/meta polja prije register-task-definition
          jq 'del(
            .status,
            .requiresAttributes,
            .compatibilities,
            .revision,
            .registeredAt,
            .registeredBy,
            .deregisteredAt,
            .taskDefinitionArn,
            .tags,
            .inferenceAccelerators
          )' td.json > td_clean.json

      - name: Replace image in TD
        run: |
          jq --arg NAME "${{ env.CONTAINER_NAME }}" --arg IMG "${IMAGE_URI}" '
            .containerDefinitions = (.containerDefinitions | map(
              if .name == $NAME then .image = $IMG | . else . end
            ))
          ' td_clean.json > td_new.json
          echo "New image for container '${{ env.CONTAINER_NAME }}': ${IMAGE_URI}"

      - name: Register new TD revision
        run: |
          NEW_TD_ARN=$(aws ecs register-task-definition \
            --cli-input-json file://td_new.json \
            --query "taskDefinition.taskDefinitionArn" --output text)
          echo "NEW_TD_ARN=$NEW_TD_ARN" >> $GITHUB_ENV
          echo "Registered: $NEW_TD_ARN"

      - name: Update service
        run: |
          aws ecs update-service \
            --cluster "${{ env.ECS_CLUSTER }}" \
            --service "${{ env.ECS_SERVICE }}" \
            --task-definition "${NEW_TD_ARN}" \
            --query "service.taskDefinition" --output text

      # ‚úÖ Post-deploy: pokreni Prisma migracije kao privremeni Fargate task (bolji debug)
      - name: Run Prisma migrate (one-off task)
        env:
          SUBNETS: ${{ env.MIGRATE_SUBNETS }}
          SECGRP: ${{ env.MIGRATE_SG }}
          # Ako nema≈° NAT u privatnim subnetima, privremeno stavi ENABLED:
          ASSIGN_PUBLIC_IP: DISABLED
        run: |
          set -euo pipefail
          echo "Running prisma migrate deploy on new TD: ${NEW_TD_ARN}"

          # 0) (debug) prika≈æi imena kontejnera iz nove TD revizije
          aws ecs describe-task-definition --task-definition "${NEW_TD_ARN}" \
            --query "taskDefinition.containerDefinitions[].name" --output text

          # Izaberi bin: lokalni prisma ili npx
          PRISMA_BIN="./node_modules/.bin/prisma"
          if [ ! -x "$PRISMA_BIN" ]; then
            PRISMA_CMD="npx prisma migrate deploy --schema=prisma/schema.prisma"
          else
            PRISMA_CMD="$PRISMA_BIN migrate deploy --schema=prisma/schema.prisma"
          fi

          # Sastavi overrides JSON preko jq (sigurno zbog navodnika)
          OVERRIDES_JSON=$(jq -n \
            --arg app "${{ env.CONTAINER_NAME }}" \
            --arg cmd "$PRISMA_CMD" \
            --arg sidecar "nginx-proxy" \
            '{
              containerOverrides: [
                { name: $app,    command: ["sh","-lc", $cmd] },
                { name: $sidecar, command: ["sh","-lc","tail -f /dev/null"] }
              ]
            }')

          # 1) pokreni task
          TASK_ARN=$(aws ecs run-task \
            --cluster "${{ env.ECS_CLUSTER }}" \
            --launch-type FARGATE \
            --task-definition "${NEW_TD_ARN}" \
            --network-configuration "awsvpcConfiguration={subnets=[$SUBNETS],securityGroups=[$SECGRP],assignPublicIp=${ASSIGN_PUBLIC_IP}}" \
            --overrides "${OVERRIDES_JSON}" \
            --region "${{ env.AWS_REGION }}" \
            --query "tasks[0].taskArn" --output text)

          echo "Started migrate task: $TASK_ARN"

          # 2) ƒçekaj STOPPED
          aws ecs wait tasks-stopped --cluster "${{ env.ECS_CLUSTER }}" --tasks "$TASK_ARN" --region "${{ env.AWS_REGION }}"

          # 3) prika≈æi sa≈æetak razloga na razini taska i kontejnera
          echo "---- Task stoppedReason ----"
          aws ecs describe-tasks \
            --cluster "${{ env.ECS_CLUSTER }}" --tasks "$TASK_ARN" --region "${{ env.AWS_REGION }}" \
            --query "tasks[0].stoppedReason" --output text || true

          echo "---- Container breakdown ----"
          aws ecs describe-tasks \
            --cluster "${{ env.ECS_CLUSTER }}" --tasks "$TASK_ARN" --region "${{ env.AWS_REGION }}" \
            --query "tasks[0].containers[].{name:name,lastStatus:lastStatus,exitCode:exitCode,reason:reason}" --output table || true

          # 4) pokupi exit code ba≈° za app kontejner
          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster "${{ env.ECS_CLUSTER }}" --tasks "$TASK_ARN" --region "${{ env.AWS_REGION }}" \
            --query "tasks[0].containers[?name=='${{ env.CONTAINER_NAME }}'].exitCode | [0]" --output text)

          echo "Migrate task exit code: ${EXIT_CODE}"

          # 5) logging info
          LOG_GROUP=$(aws ecs describe-task-definition --task-definition "${NEW_TD_ARN}" \
            --query "taskDefinition.containerDefinitions[?name=='${{ env.CONTAINER_NAME }}'].logConfiguration.options.\"awslogs-group\" | [0]" --output text)
          LOG_PREFIX=$(aws ecs describe-task-definition --task-definition "${NEW_TD_ARN}" \
            --query "taskDefinition.containerDefinitions[?name=='${{ env.CONTAINER_NAME }}'].logConfiguration.options.\"awslogs-stream-prefix\" | [0]" --output text)
          TASK_ID=${TASK_ARN##*/}

          # 6) fail ako nema urednog exit code-a
          if [ -z "$EXIT_CODE" ] || [ "$EXIT_CODE" = "None" ] || [ "$EXIT_CODE" != "0" ]; then
            echo "Prisma migrate failed (exit=${EXIT_CODE:-unset})"

            # poku≈°aj specifiƒçni stream
            if [ -n "$LOG_GROUP" ] && [ -n "$LOG_PREFIX" ] && [ "$LOG_GROUP" != "None" ] && [ "$LOG_PREFIX" != "None" ]; then
              aws logs tail "$LOG_GROUP" --log-stream-names "$LOG_PREFIX/${{ env.CONTAINER_NAME }}/$TASK_ID" --since 30m --region "${{ env.AWS_REGION }}" || \
              aws logs tail "$LOG_GROUP" --since 30m --region "${{ env.AWS_REGION }}" || true
            fi

            exit 1
          fi

          echo "Prisma migrate finished successfully."
